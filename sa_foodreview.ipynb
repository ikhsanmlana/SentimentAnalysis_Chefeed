{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 09:50:22.482482: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-01 09:50:22.482503: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewId</th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>AuthorId</th>\n",
       "      <th>AuthorName</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>DateSubmitted</th>\n",
       "      <th>DateModified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>992</td>\n",
       "      <td>2008</td>\n",
       "      <td>gayg msft</td>\n",
       "      <td>5</td>\n",
       "      <td>better than any you can get at a restaurant!</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "      <td>2000-01-25T21:44:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>4384</td>\n",
       "      <td>1634</td>\n",
       "      <td>Bill Hilbrich</td>\n",
       "      <td>4</td>\n",
       "      <td>I cut back on the mayo, and made up the differ...</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "      <td>2001-10-17T16:49:59Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>4523</td>\n",
       "      <td>2046</td>\n",
       "      <td>Gay Gilmore ckpt</td>\n",
       "      <td>2</td>\n",
       "      <td>i think i did something wrong because i could ...</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "      <td>2000-02-25T09:00:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>7435</td>\n",
       "      <td>1773</td>\n",
       "      <td>Malarkey Test</td>\n",
       "      <td>5</td>\n",
       "      <td>easily the best i have ever had.  juicy flavor...</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "      <td>2000-03-13T21:15:00Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>2085</td>\n",
       "      <td>Tony Small</td>\n",
       "      <td>5</td>\n",
       "      <td>An excellent dish.</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "      <td>2000-03-28T12:51:00Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReviewId  RecipeId  AuthorId        AuthorName  Rating  \\\n",
       "0         2       992      2008         gayg msft       5   \n",
       "1         7      4384      1634     Bill Hilbrich       4   \n",
       "2         9      4523      2046  Gay Gilmore ckpt       2   \n",
       "3        13      7435      1773     Malarkey Test       5   \n",
       "4        14        44      2085        Tony Small       5   \n",
       "\n",
       "                                              Review         DateSubmitted  \\\n",
       "0       better than any you can get at a restaurant!  2000-01-25T21:44:00Z   \n",
       "1  I cut back on the mayo, and made up the differ...  2001-10-17T16:49:59Z   \n",
       "2  i think i did something wrong because i could ...  2000-02-25T09:00:00Z   \n",
       "3  easily the best i have ever had.  juicy flavor...  2000-03-13T21:15:00Z   \n",
       "4                                 An excellent dish.  2000-03-28T12:51:00Z   \n",
       "\n",
       "           DateModified  \n",
       "0  2000-01-25T21:44:00Z  \n",
       "1  2001-10-17T16:49:59Z  \n",
       "2  2000-02-25T09:00:00Z  \n",
       "3  2000-03-13T21:15:00Z  \n",
       "4  2000-03-28T12:51:00Z  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('reviews.csv') \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReviewId           0\n",
       "RecipeId           0\n",
       "AuthorId           0\n",
       "AuthorName         0\n",
       "Rating             0\n",
       "Review           214\n",
       "DateSubmitted      0\n",
       "DateModified       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReviewId         0\n",
       "RecipeId         0\n",
       "AuthorId         0\n",
       "AuthorName       0\n",
       "Rating           0\n",
       "Review           0\n",
       "DateSubmitted    0\n",
       "DateModified     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(subset=['Review'])\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_reviews'] = df['Review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         better than any you can get at a restaurant!\n",
       "1    i cut back on the mayo, and made up the differ...\n",
       "2    i think i did something wrong because i could ...\n",
       "3    easily the best i have ever had.  juicy flavor...\n",
       "4                                   an excellent dish.\n",
       "Name: cleaned_reviews, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_reviews'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1286053/3349767945.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['cleaned_reviews'] = df['cleaned_reviews'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          better than any you can get at a restaurant\n",
       "1    i cut back on the mayo and made up the differe...\n",
       "2    i think i did something wrong because i could ...\n",
       "3    easily the best i have ever had  juicy flavorf...\n",
       "4                                    an excellent dish\n",
       "Name: cleaned_reviews, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_reviews'] = df['cleaned_reviews'].str.replace('[^\\w\\s]','') \n",
    "df['cleaned_reviews'].head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(text):\n",
    "    filtered_words = []\n",
    "    for x in text: \n",
    "        if x not in stop_words:\n",
    "            filtered_words.append(x)\n",
    "    \n",
    "    return filtered_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_reviews'] = df['cleaned_reviews'].apply(lambda x: x.split(\" \"))\n",
    "df['cleaned_reviews'] = df['cleaned_reviews'].apply(lambda x: remove_sw(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            [better, get, restaurant]\n",
       "1    [cut, back, mayo, made, difference, sour, crea...\n",
       "2    [think, something, wrong, could, taste, cornst...\n",
       "3    [easily, best, ever, , juicy, flavorful, dry, ...\n",
       "4                                    [excellent, dish]\n",
       "Name: cleaned_reviews, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_reviews'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_reviews</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[better, get, restaurant]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[cut, back, mayo, made, difference, sour, crea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[think, something, wrong, could, taste, cornst...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[easily, best, ever, , juicy, flavorful, dry, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[excellent, dish]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401977</th>\n",
       "      <td>[disappointed, couldnt, wait, make, husband, b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401978</th>\n",
       "      <td>[nothing, drain, dont, heat, liquids, put, mil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401979</th>\n",
       "      <td>[good, base, recipe, someone, start, quadruple...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401980</th>\n",
       "      <td>[thank, much, amazing, recipe, lived, kenai, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401981</th>\n",
       "      <td>[cant, say, enough, recipe, best, pot, roast, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1401768 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           cleaned_reviews  is_positive\n",
       "0                                [better, get, restaurant]            1\n",
       "1        [cut, back, mayo, made, difference, sour, crea...            1\n",
       "2        [think, something, wrong, could, taste, cornst...            0\n",
       "3        [easily, best, ever, , juicy, flavorful, dry, ...            1\n",
       "4                                        [excellent, dish]            1\n",
       "...                                                    ...          ...\n",
       "1401977  [disappointed, couldnt, wait, make, husband, b...            0\n",
       "1401978  [nothing, drain, dont, heat, liquids, put, mil...            1\n",
       "1401979  [good, base, recipe, someone, start, quadruple...            1\n",
       "1401980  [thank, much, amazing, recipe, lived, kenai, s...            1\n",
       "1401981  [cant, say, enough, recipe, best, pot, roast, ...            1\n",
       "\n",
       "[1401768 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "df['is_positive'] = np.where(df['Rating']<3, 0, 1) \n",
    "\n",
    "df[['cleaned_reviews','is_positive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "661"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_reviews'] = df['cleaned_reviews'].apply(lambda x: list(filter(None,x)))\n",
    "        \n",
    "\n",
    "max_word = max(df.cleaned_reviews, key=len) \n",
    "len(max_word)\n",
    "\n",
    "# import urllib.request \n",
    "\n",
    "# url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
    "# filename = 'glove.6b.zip'\n",
    "# urllib.request.urlretrieve(url,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df.cleaned_reviews.str.len() < 100]\n",
    "\n",
    "max_word = max(df.cleaned_reviews, key=len) \n",
    "len(max_word)\n",
    "\n",
    "# import zipfile \n",
    "\n",
    "# with zipfile.ZipFile('glove.6b.zip', 'r') as zip: \n",
    "#     zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_vectors = dict()\n",
    "\n",
    "# def add_wordvector(dict, filename):\n",
    "#     with open(filename, 'r', encoding='utf8') as f:\n",
    "#         for line in f.readlines():\n",
    "#             line = line.split(' ')\n",
    "\n",
    "#             try:\n",
    "#                 if line[0] not in stop_words:\n",
    "#                     dict[line[0]] = np.array(line[1:], dtype=float)\n",
    "#             except:\n",
    "#                 continue \n",
    "\n",
    "def add_wordvector(filename):\n",
    "    with open(filename, 'r', encoding='utf8') as f:\n",
    "        wordToGloveVector = {}\n",
    "        wordToIndex = {}\n",
    "        indexToWord = {}\n",
    "        \n",
    "        \n",
    "        for line in f.readlines():\n",
    "            line = line.split(' ')\n",
    "\n",
    "            try:\n",
    "                if line[0] not in stop_words:\n",
    "                    wordToGloveVector[line[0]] = np.array(line[1:], dtype=float)\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        tokens = sorted(wordToGloveVector.keys())\n",
    "        for i, word in enumerate(tokens):\n",
    "            kerasIdx = i + 1 # 0 is reserved for masking n Keras \n",
    "            wordToIndex[word] = kerasIdx \n",
    "            indexToWord[kerasIdx] = word \n",
    "            \n",
    "    return wordToGloveVector, wordToIndex, indexToWord\n",
    "            \n",
    "wordToGloveVector, wordToIndex, indexToWord = add_wordvector('glove.6B.50d.txt')\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.5586e-01,  5.2130e-01, -6.1070e-01, -3.0131e-01,  9.4862e-01,\n",
       "       -3.1539e-01, -5.9831e-01,  1.2188e-01, -3.1943e-02,  5.5695e-01,\n",
       "       -1.0621e-01,  6.3399e-01, -4.7340e-01, -7.5895e-02,  3.8247e-01,\n",
       "        8.1569e-02,  8.2214e-01,  2.2220e-01, -8.3764e-03, -7.6620e-01,\n",
       "       -5.6253e-01,  6.1759e-01,  2.0292e-01, -4.8598e-02,  8.7815e-01,\n",
       "       -1.6549e+00, -7.7418e-01,  1.5435e-01,  9.4823e-01, -3.9520e-01,\n",
       "        3.7302e+00,  8.2855e-01, -1.4104e-01,  1.6395e-02,  2.1115e-01,\n",
       "       -3.6085e-02, -1.5587e-01,  8.6583e-01,  2.6309e-01, -7.1015e-01,\n",
       "       -3.6770e-02,  1.8282e-03, -1.7704e-01,  2.7032e-01,  1.1026e-01,\n",
       "        1.4133e-01, -5.7322e-02,  2.7207e-01,  3.1305e-01,  9.2771e-01])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordToGloveVector)\n",
    "\n",
    "wordToGloveVector['good'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164285"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordToIndex['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['better', 'get', 'restaurant']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_token(text):\n",
    "    lemmatized_token = [lemmatizer.lemmatize(x) for x in text] \n",
    "    useful_token = [x for x in lemmatized_token if x in wordToGloveVector]\n",
    "\n",
    "    return useful_token \n",
    "\n",
    "word_token = df['cleaned_reviews'][0] \n",
    "\n",
    "lemmatize_token(word_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_vector(text, token_dict=wordToGloveVector):\n",
    "    processed_token = lemmatize_token(text)\n",
    "\n",
    "    vectors = []\n",
    "\n",
    "    for x in processed_token:\n",
    "        if x not in wordToGloveVector:\n",
    "            continue \n",
    "        \n",
    "        vectors.append(token_dict[x]) \n",
    "\n",
    "    return np.array(vectors, dtype=float) \n",
    "\n",
    "def text_to_indices(text):\n",
    "    processed_token = lemmatize_token(text)\n",
    "    \n",
    "    wordToIndices = []\n",
    "    \n",
    "    for x in processed_token:\n",
    "        if x not in wordToGloveVector:\n",
    "            continue \n",
    "        \n",
    "        wordToIndices.append(wordToIndex[x])\n",
    "        \n",
    "        \n",
    "    return np.array(wordToIndices, dtype=int)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vectorize_review(df):\n",
    "#     label = df['is_positive'].to_numpy().astype(int)\n",
    "\n",
    "#     vectorized_text = []\n",
    "\n",
    "#     for text in df['cleaned_reviews']:\n",
    "#         text_as_vector = text_to_vector(text) \n",
    "\n",
    "#         if text_as_vector.shape[0] == 0:\n",
    "#             text_as_vector = np.zeros(shape=(1,50)) \n",
    "\n",
    "#         vectorized_text.append(text_as_vector)\n",
    "\n",
    "#     return vectorized_text, label\n",
    "\n",
    "def map_input_output(df):\n",
    "    label = df['is_positive'].to_numpy().astype(int)\n",
    "\n",
    "    input_text = []\n",
    "\n",
    "    for text in df['cleaned_reviews']:\n",
    "#         text_as_indices = text_to_indices(text)\n",
    "    \n",
    "#         if text_as_indices.shape[0] == 0:\n",
    "#             text_as_indices = np.zeros(shape=(1))\n",
    "\n",
    "        processed_token = lemmatize_token(text)\n",
    "        \n",
    "#         if len(processed_token) == False:\n",
    "#             processed_token = np.zeros(shape=(1)) \n",
    "\n",
    "        input_text.append(processed_token)\n",
    "        \n",
    "    return input_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350481, 75103, 75104)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df.sample(frac=.36, random_state=1)\n",
    "train_df.reset_index(drop=True, inplace=True) \n",
    "\n",
    "split_index_1 = int(len(train_df) * 0.7)\n",
    "split_index_2 = int(len(train_df) * 0.85)\n",
    "\n",
    "train_df, val_df, test_df = train_df[:split_index_1], train_df[split_index_1:split_index_2], train_df[split_index_2:]\n",
    "\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350481, 18, 14)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = map_input_output(train_df) \n",
    "\n",
    "len(x_train), len(x_train[0]), len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    350481.000000\n",
       "mean         25.145871\n",
       "std          16.206878\n",
       "min           0.000000\n",
       "25%          13.000000\n",
       "50%          22.000000\n",
       "75%          33.000000\n",
       "max          99.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_len = []\n",
    "\n",
    "for i in range (len(x_train)):\n",
    "    token_len.append(len(x_train[i])) \n",
    "\n",
    "pd.Series(token_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "words_to_index = tokenizer.word_index\n",
    "\n",
    "# words_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_indices = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "# x_train_indices\n",
    "\n",
    "# wordToIndex['recipe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "def createPretrainedEmbeddingLayer(wordToGloveVector, wordIndex):\n",
    "    vocabLen = len(wordIndex) + 1 \n",
    "    embDim = 50\n",
    "    \n",
    "    embeddingMatrix = np.zeros((vocabLen, embDim)) \n",
    "    for word, index in wordIndex.items():\n",
    "        embeddingMatrix[index, :] = wordToGloveVector[word]\n",
    "        \n",
    "    embeddingLayer = Embedding(vocabLen, embDim, input_length=100, embeddings_initializer=tf.keras.initializers.Constant(embeddingMatrix), trainable=False)\n",
    "    \n",
    "    return embeddingLayer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.3259e-02, -1.0774e-01,  1.9567e-01,  3.3223e-02,  7.4798e-02,\n",
       "       -2.4640e-01, -3.7363e-01,  2.3737e-01, -4.3820e-02,  7.5206e-01,\n",
       "       -6.8002e-01,  1.4228e-01, -5.1052e-02,  1.2944e-01,  6.1359e-02,\n",
       "        1.3980e-01,  6.7984e-01, -5.8867e-01,  1.1884e-01, -1.0120e+00,\n",
       "       -4.2694e-01,  1.5890e-01, -7.4903e-02,  5.3789e-01,  1.0589e+00,\n",
       "       -1.3319e+00, -3.8969e-01,  8.3434e-02,  1.2894e+00, -7.9956e-01,\n",
       "        3.0825e+00,  3.7201e-01, -4.9131e-01,  1.5977e-01,  4.3779e-01,\n",
       "        1.2637e-01,  1.1982e-01,  9.9909e-01, -2.4347e-01, -7.7499e-01,\n",
       "        4.7008e-01,  1.7949e-01,  2.6817e-01,  6.6041e-01,  2.0982e-01,\n",
       "        3.2454e-01,  6.6948e-01, -2.9127e-03,  6.3353e-01,  5.2693e-01])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gloveEmbeddingLayer = createPretrainedEmbeddingLayer(wordToGloveVector, words_to_index)\n",
    "\n",
    "wordToGloveVector['easy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[1133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recipe',\n",
       " 'us',\n",
       " '1',\n",
       " 'cup',\n",
       " 'cornflakes',\n",
       " 'instead',\n",
       " '2',\n",
       " '12',\n",
       " 'cup',\n",
       " 'flour',\n",
       " 'shortening',\n",
       " 'cut',\n",
       " 'flour',\n",
       " 'right',\n",
       " 'good',\n",
       " 'lol',\n",
       " 'thank',\n",
       " 'posting']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "# x_train\n",
    "\n",
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts_to_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m x_train \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mpad_sequences(x_train, maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      5\u001b[0m x_train\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/keras/preprocessing/text.py:337\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtexts_to_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts):\n\u001b[1;32m    326\u001b[0m   \u001b[38;5;124;03m\"\"\"Transforms each text in texts to a sequence of integers.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m  Only top `num_words-1` most frequent words will be taken into account.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m      A list of sequences.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtexts_to_sequences_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/keras/preprocessing/text.py:366\u001b[0m, in \u001b[0;36mTokenizer.texts_to_sequences_generator\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 366\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[43mtext_to_word_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manalyzer(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/test/lib/python3.10/site-packages/keras/preprocessing/text.py:72\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a text to a sequence of words (or tokens).\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mDeprecated: `tf.keras.preprocessing.text.text_to_word_sequence` does not\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    A list of words (or tokens).\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[0;32m---> 72\u001b[0m   input_text \u001b[38;5;241m=\u001b[39m \u001b[43minput_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     74\u001b[0m translate_dict \u001b[38;5;241m=\u001b[39m {c: split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m filters}\n\u001b[1;32m     75\u001b[0m translate_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(translate_dict)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "x_train = tf.keras.utils.pad_sequences(x_train, maxlen=100, dtype='float32', padding='post') \n",
    "\n",
    "x_train.shape\n",
    "# tf.keras.utils.pad_sequences([[1, 2, 3], [3, 4, 5, 6], [7, 8]], dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0]), len(x_train[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350481,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75103, 100), (75103,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val, y_val = map_input_output(val_df)\n",
    "\n",
    "x_val = tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "x_val = tf.keras.utils.pad_sequences(x_val, maxlen=100, dtype='float32', padding='post')\n",
    "\n",
    "x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75104, 100), (75104,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test, y_test = map_input_output(test_df)\n",
    "\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "x_test = tf.keras.utils.pad_sequences(x_test, maxlen=100, dtype='float32', padding='post')\n",
    "\n",
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([])\n",
    "\n",
    "# # model.add(layers.Input(shape=(100,50))) \n",
    "# model.add(gloveEmbeddingLayer)\n",
    "# model.add(layers.LSTM(460, return_sequences=True))\n",
    "# model.add(layers.Dropout(0.2))\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(128, activation='relu'))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           1654500   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 100, 46)           13524     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 46)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 4601      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,672,625\n",
      "Trainable params: 18,125\n",
      "Non-trainable params: 1,654,500\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-30 19:28:21.525773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-30 19:28:21.526708: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-30 19:28:21.526750: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-30 19:28:21.526786: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-30 19:28:21.528756: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-30 19:28:21.528801: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-30 19:28:21.528842: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-30 19:28:21.528849: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-30 19:28:21.529291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "modelGRU = Sequential([])\n",
    "\n",
    "# modelGRU.add(layers.Input(shape=(100,50)))\n",
    "modelGRU.add(gloveEmbeddingLayer)\n",
    "modelGRU.add(layers.GRU(46, return_sequences=True))\n",
    "modelGRU.add(layers.Dropout(0.2))\n",
    "modelGRU.add(layers.Flatten())\n",
    "modelGRU.add(layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "modelGRU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelRNN = Sequential([])\n",
    "\n",
    "# # modelRNN.add(layers.Input(shape=(100,50)))\n",
    "# modelRNN.add(gloveEmbeddingLayer)\n",
    "# modelRNN.add(layers.SimpleRNN(68, return_sequences=True))\n",
    "# modelRNN.add(layers.Dropout(0.2))\n",
    "# # modelRNN.add(layers.SimpleRNN(68)) \n",
    "# modelRNN.add(layers.Flatten())\n",
    "# modelRNN.add(layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "# modelRNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from tensorflow.keras.metrics import AUC \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "\n",
    "# model_cp = ModelCheckpoint('model_embed_reludense/', save_best_only=True)\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(), metrics=['accuracy', AUC(name='auc')]) \n",
    "\n",
    "# modelrnn_cp = ModelCheckpoint('modelrnn_embed/', save_best_only=True)\n",
    "# modelRNN.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(), metrics=['accuracy', AUC(name='auc')])\n",
    "\n",
    "modelgru_cp = ModelCheckpoint('modelgru_embed2/', save_best_only=True)\n",
    "modelGRU.compile(optimizer=Adam(learning_rate=0.001), loss=BinaryCrossentropy(), metrics=['accuracy', AUC(name='auc')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    322925\n",
       "0     27556\n",
       "Name: is_positive, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_freq = pd.value_counts(train_df['is_positive']) \n",
    "class_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 6.359431702714472, 1: 0.5426662537740962}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {0: class_freq.sum() / (class_freq[0] * 2), 1: class_freq.sum() / (class_freq[1] * 2)}   #model1\n",
    "\n",
    "# weights = {0: class_freq.sum() / class_freq[0], 1: class_freq.sum() / class_freq[1] } #model2\n",
    "\n",
    "# weights = {0: (class_freq.sum() / (class_freq[0]))+3, 1: class_freq.sum() / (class_freq[1])} #model3\n",
    "\n",
    "weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "10952/10953 [============================>.] - ETA: 0s - loss: 0.5603 - accuracy: 0.7209 - auc: 0.7841"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelgru_embed2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelgru_embed2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10953/10953 [==============================] - 346s 32ms/step - loss: 0.5603 - accuracy: 0.7209 - auc: 0.7841 - val_loss: 0.6228 - val_accuracy: 0.6594 - val_auc: 0.8129\n",
      "Epoch 2/15\n",
      "10952/10953 [============================>.] - ETA: 0s - loss: 0.5236 - accuracy: 0.7623 - auc: 0.8150"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelgru_embed2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: modelgru_embed2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10953/10953 [==============================] - 342s 31ms/step - loss: 0.5236 - accuracy: 0.7623 - auc: 0.8150 - val_loss: 0.4106 - val_accuracy: 0.8378 - val_auc: 0.8236\n",
      "Epoch 3/15\n",
      "10953/10953 [==============================] - 339s 31ms/step - loss: 0.5102 - accuracy: 0.7758 - auc: 0.8253 - val_loss: 0.4303 - val_accuracy: 0.8283 - val_auc: 0.8276\n",
      "Epoch 4/15\n",
      "10953/10953 [==============================] - 339s 31ms/step - loss: 0.5010 - accuracy: 0.7837 - auc: 0.8323 - val_loss: 0.4863 - val_accuracy: 0.7864 - val_auc: 0.8291\n",
      "Epoch 5/15\n",
      "10953/10953 [==============================] - 342s 31ms/step - loss: 0.4916 - accuracy: 0.7903 - auc: 0.8396 - val_loss: 0.4702 - val_accuracy: 0.7969 - val_auc: 0.8322\n",
      "Epoch 6/15\n",
      "10953/10953 [==============================] - 343s 31ms/step - loss: 0.4859 - accuracy: 0.7933 - auc: 0.8438 - val_loss: 0.5571 - val_accuracy: 0.7446 - val_auc: 0.8328\n",
      "Epoch 7/15\n",
      "10953/10953 [==============================] - 341s 31ms/step - loss: 0.4798 - accuracy: 0.7961 - auc: 0.8481 - val_loss: 0.4667 - val_accuracy: 0.8033 - val_auc: 0.8289\n",
      "Epoch 8/15\n",
      "10953/10953 [==============================] - 342s 31ms/step - loss: 0.4757 - accuracy: 0.7971 - auc: 0.8511 - val_loss: 0.4325 - val_accuracy: 0.8218 - val_auc: 0.8282\n",
      "Epoch 9/15\n",
      "10953/10953 [==============================] - 344s 31ms/step - loss: 0.4708 - accuracy: 0.7983 - auc: 0.8552 - val_loss: 0.4706 - val_accuracy: 0.7966 - val_auc: 0.8292\n",
      "Epoch 10/15\n",
      "10953/10953 [==============================] - 340s 31ms/step - loss: 0.4650 - accuracy: 0.8015 - auc: 0.8586 - val_loss: 0.5345 - val_accuracy: 0.7439 - val_auc: 0.8239\n",
      "Epoch 11/15\n",
      "10953/10953 [==============================] - 344s 31ms/step - loss: 0.4628 - accuracy: 0.7998 - auc: 0.8601 - val_loss: 0.5928 - val_accuracy: 0.7066 - val_auc: 0.8251\n",
      "Epoch 12/15\n",
      "10953/10953 [==============================] - 344s 31ms/step - loss: 0.4581 - accuracy: 0.8025 - auc: 0.8636 - val_loss: 0.5277 - val_accuracy: 0.7484 - val_auc: 0.8266\n",
      "Epoch 13/15\n",
      "10953/10953 [==============================] - 343s 31ms/step - loss: 0.4550 - accuracy: 0.8027 - auc: 0.8661 - val_loss: 0.4534 - val_accuracy: 0.8063 - val_auc: 0.8229\n",
      "Epoch 14/15\n",
      "10953/10953 [==============================] - 339s 31ms/step - loss: 0.4506 - accuracy: 0.8041 - auc: 0.8687 - val_loss: 0.5167 - val_accuracy: 0.7662 - val_auc: 0.8177\n",
      "Epoch 15/15\n",
      "10953/10953 [==============================] - 345s 32ms/step - loss: 0.4488 - accuracy: 0.8033 - auc: 0.8698 - val_loss: 0.4613 - val_accuracy: 0.8044 - val_auc: 0.8209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc3ca25330>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(x_train, y_train, validation_data=(x_val,y_val), epochs=15, callbacks=[model_cp], class_weight=weights) \n",
    "\n",
    "# modelRNN.fit(x_train, y_train, validation_data=(x_val,y_val), epochs=15, callbacks=[modelrnn_cp], class_weight=weights) \n",
    "\n",
    "modelGRU.fit(x_train, y_train, validation_data=(x_val,y_val), epochs=15, callbacks=[modelgru_cp], class_weight=weights) \n",
    "\n",
    "# modelGRU.fit(x_train, y_train, validation_data=(x_val,y_val), epochs=15, class_weight=weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelGRU.save('gru_embed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 09:56:03.663412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-01 09:56:03.664577: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-01 09:56:03.664630: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-01 09:56:03.664684: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-01 09:56:03.666571: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-01 09:56:03.666625: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-07-01 09:56:03.666675: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-01 09:56:03.666684: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-01 09:56:03.666995: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model \n",
    "\n",
    "best_model = load_model('modelgru_embed2/') \n",
    "\n",
    "# best_modelrnn = load_model('modelrnn/')\n",
    "\n",
    "# best_modelgru = load_model('modelgru/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2347/2347 [==============================] - 17s 7ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.62      0.37      5871\n",
      "           1       0.96      0.85      0.91     69233\n",
      "\n",
      "    accuracy                           0.84     75104\n",
      "   macro avg       0.61      0.73      0.64     75104\n",
      "weighted avg       0.91      0.84      0.86     75104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = (best_model.predict(x_test) > 0.5)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2347/2347 [==============================] - 17s 7ms/step - loss: 0.4140 - accuracy: 0.8358 - auc: 0.8171\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.41402214765548706, 0.8358409404754639, 0.8171298503875732]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           1654500   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 100, 46)           13524     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 46)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 4601      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,672,625\n",
      "Trainable params: 18,125\n",
      "Non-trainable params: 1,654,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model2 = load_model('model2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 100, 460)          940240    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100, 460)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 46000)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 46001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 986,241\n",
      "Trainable params: 986,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_embed = load_model('model_embed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 50)           19992600  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100, 460)          940240    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 460)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 46000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 46001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,978,841\n",
      "Trainable params: 986,241\n",
      "Non-trainable params: 19,992,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_embed.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
